{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4yYIhuphpTyt"
   },
   "source": [
    "# Week 1: CUDA operations in PyTorch. Introduction to benchmarking.\n",
    "In this seminar, we'll learn a bit more about the things one needs to keep in mind when using GPU computations, both in general and in PyTorch. We'll also see a couple of examples on benchmarking code in Python; again, there are some caveats with CUDA.\n",
    "\n",
    "First, let's import PyTorch and get some information about the currently used device:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "089F8Q-yK9-w",
    "outputId": "ad1e77f9-8ab9-46c1-ca13-c17b0af561d7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "C9iz9uYOn5JY",
    "outputId": "ab0a5810-88c6-4caa-c135-501d0fc1b8f9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_CudaDeviceProperties(name='NVIDIA RTX 4000 Ada Generation', major=8, minor=9, total_memory=20146MB, multi_processor_count=48)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.get_device_properties(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i3_7snMwx5Yq"
   },
   "source": [
    "## Memory allocation\n",
    "As discussed in the lecture, GPU memory is separate from the CPU memory and needs to be explicitly allocated, triggering a host-device synchronization. PyTorch uses a caching memory allocator to repurpose already available but unused memory fragments.\n",
    "\n",
    "See this example: we allocate the memory inside the function scope, so the tensor is deleted as soon as the scope is left:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "Maafa5Nehba-"
   },
   "outputs": [],
   "source": [
    "def allocate_empty_tensor(dim_size):\n",
    "    a = torch.empty(4096, dim_size, dtype=torch.float32, device=\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "LhI9_OvaoOIi"
   },
   "outputs": [],
   "source": [
    "allocate_empty_tensor(2048)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XAxd3nkJ3UsK"
   },
   "source": [
    "Printing the allocated memory size gives us an expected number of zero bytes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rez0XGH6gGF9",
    "outputId": "ed4ea74d-6256-47b9-840e-a07e468601c9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.memory_allocated(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qzcQ2ohm3d2v"
   },
   "source": [
    "However, the GPU memory is still in use by the process; this is shown by `torch.cuda.memory_reserved` or `nvidia-smi` from the terminal. As a result, working in a shared GPU environment can leave a lot of unused yet allocated memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "H6IOufbXoQwI",
    "outputId": "f3574e09-0fa6-43c8-b38e-7ad884a06bd8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33554432"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.memory_reserved()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33554432"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "4096 * 2048 * 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1iM6LZBWIDT1",
    "outputId": "dd8e168f-ab71-4b95-92b5-025694168530"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Aug 29 15:57:01 2024       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA RTX 4000 Ada Gene...    On  |   00000000:01:00.0 Off |                  Off |\n",
      "| 30%   35C    P2             14W /  130W |     196MiB /  20475MiB |      1%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JUK6clp83nGu"
   },
   "source": [
    "Let's clear the cache now and see what happens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SSPz5VWYopb7",
    "outputId": "e4961e19-fc2f-42d4-f19b-83cbc7c1c9ae"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "torch.cuda.memory_reserved()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m3m-rcF64j1l"
   },
   "source": [
    "Note that this operation triggers a CPU-GPU synchronization, and thus using it in your code can significantly hurt the performance. It's almost always better to carefully manage the lifetime of your GPU tensors and avoid excessive allocations than to empty the cache.\n",
    "\n",
    "Now, let's see how this cache is reused by allocating two tensors in a row: first a larger one, then a smaller one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8YHdjvrn32_F",
    "outputId": "418c905e-9ed1-46b7-a36e-a39e455390ad"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33554432"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "allocate_empty_tensor(2048)\n",
    "torch.cuda.memory_reserved()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4qKRSbS634vg",
    "outputId": "47015a3e-119e-4500-b78e-679a6d2759ad"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33554432"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "allocate_empty_tensor(1024)\n",
    "torch.cuda.memory_reserved()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MpLYUugy4hTW"
   },
   "source": [
    "As expected, we reuse the cache, since the array fits into the allocated chunk.\n",
    "\n",
    "However, if we attempt to do this with a larger tensor (3072 elements in the second dimension instead of 2048), we observe something different:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aOe0In1I3_ix",
    "outputId": "687200cf-bf9a-4fe1-8c78-9cdbd3cbf2c4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "83886080"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "allocate_empty_tensor(3072)\n",
    "torch.cuda.memory_reserved()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "83886080"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "3072 * 4096 * 4 + 2048 * 4096 * 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ayR2vJmL4g4a"
   },
   "source": [
    "What happened? The chunk of memory that was allocated for a 4096x2048 array did not fit a tensor of size 4096x3072: thus, PyTorch needed to allocate an additional segment of a sufficient size while keeping the previous one allocated (in case the user creates a smaller tensor later at some point).\n",
    "\n",
    "In practice, this means that if your code is dealing with tensors of dynamic size (changing batch sizes, sequence lengths or image resolutions), it is recommended to first warm up the cache (**note from myself**: I noticed quite the opposite. If I allocated the memory in advance, the program failed with CUDA OOM) by allocating tensors for the largest expected input. Otherwise, in the worst case you allocate a quadratic amount of memory with respect to the largest input size instead of a linear one.\n",
    "\n",
    "You can also view more detailed allocation statistics by running `torch.cuda.memory_stats()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.empty(2048 * 4096, device=\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After deleting a: 33554432\n",
      "After deleting b: 0\n",
      "After doing garbage collection: 0\n"
     ]
    }
   ],
   "source": [
    "b = a\n",
    "del a\n",
    "print(f\"After deleting a: {torch.cuda.memory_allocated()}\")\n",
    "del b\n",
    "print(f\"After deleting b: {torch.cuda.memory_allocated()}\")\n",
    "gc.collect()\n",
    "print(f\"After doing garbage collection: {torch.cuda.memory_allocated()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "leOBOx9OmCqP",
    "outputId": "32a2364f-f8c8-4f11-db9e-d0f33f33acb1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "0\n",
      "1\n",
      "83886080\n"
     ]
    }
   ],
   "source": [
    "memory_stats = torch.cuda.memory_stats()\n",
    "print(memory_stats[\"active.all.allocated\"])\n",
    "print(memory_stats[\"active.all.current\"])\n",
    "print(memory_stats[\"active.all.peak\"])\n",
    "print(memory_stats[\"reserved_bytes.all.current\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BtsyuLdFI6N9",
    "outputId": "a3e8ae45-4bdb-40e8-afdd-1f39c0ca23a6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "print(torch.cuda.memory_stats()[\"reserved_bytes.all.current\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PexpzQZ_xPss"
   },
   "source": [
    "## Benchmarking intro\n",
    "The simplest way to check the performance impact of any change is to compare the runtime of code with and without it. Here, we will consider a simple way of doing this in Python.\n",
    "\n",
    "First, let's define two functions that compute a batched version of a dot product for two matrices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "YH5Wbu_GLLN0"
   },
   "outputs": [],
   "source": [
    "def batched_dot_mul_sum(a, b):\n",
    "    \"\"\"Computes batched dot by multiplying and summing\"\"\"\n",
    "    return a.mul(b).sum(-1)\n",
    "\n",
    "\n",
    "def batched_dot_bmm(a, b):\n",
    "    \"\"\"Computes batched dot by reducing to bmm\"\"\"\n",
    "    a = a.reshape(-1, 1, a.shape[-1])\n",
    "    b = b.reshape(-1, b.shape[-1], 1)\n",
    "    return torch.bmm(a, b).flatten(-3)\n",
    "\n",
    "\n",
    "# Input for benchmarking\n",
    "x = torch.randn(10000, 64)\n",
    "\n",
    "# Ensure that both functions compute the same output\n",
    "assert batched_dot_mul_sum(x, x).allclose(batched_dot_bmm(x, x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7iJ6zelnyE37"
   },
   "source": [
    "To conduct microbenchmarks by running the code hundreds of times and measuring the average execution time, you can use the built-in [timeit](https://docs.python.org/3/library/timeit.html) module. Simply create a Timer object with relevant arguments and call `.timeit()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rKAVpQ1hLMxI",
    "outputId": "91265271-d3d9-49c1-8da2-51475bb4f190"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mul_sum(x, x):  162.4 us\n",
      "bmm(x, x):       78.7 us\n"
     ]
    }
   ],
   "source": [
    "import timeit\n",
    "\n",
    "t0 = timeit.Timer(\n",
    "    stmt=\"batched_dot_mul_sum(x, x)\",\n",
    "    setup=\"from __main__ import batched_dot_mul_sum\",\n",
    "    globals={\"x\": x},\n",
    ")\n",
    "\n",
    "t1 = timeit.Timer(\n",
    "    stmt=\"batched_dot_bmm(x, x)\",\n",
    "    setup=\"from __main__ import batched_dot_bmm\",\n",
    "    globals={\"x\": x},\n",
    ")\n",
    "\n",
    "print(f\"mul_sum(x, x):  {t0.timeit(100) / 100 * 1e6:>5.1f} us\")\n",
    "print(f\"bmm(x, x):      {t1.timeit(100) / 100 * 1e6:>5.1f} us\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OFm8rggSK3FW"
   },
   "source": [
    "In IPython, there exist line and cell timeit [magics](https://ipython.readthedocs.io/en/stable/interactive/magics.html#cell-magics). They can be more convenient for smaller cases but allow you a bit less control over the setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NM0hjbs3JxLF",
    "outputId": "e14b1636-e73e-4b10-96d5-5098c0b43988"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "529 μs ± 47.7 μs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit batched_dot_mul_sum(x, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a28ZIFy_KZl0",
    "outputId": "e85e659d-ea4a-4682-ac04-6f5de2d8e83a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "230 μs ± 43 μs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit batched_dot_bmm(x, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F1WqSrVJyEAd"
   },
   "source": [
    "[torch.utils.benchmark](https://pytorch.org/docs/stable/benchmark_utils.html) copies the API of timeit while helping the user avoid common mistakes.\n",
    "\n",
    "First, let's run it on the same code and data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GF1XAx1ULSch",
    "outputId": "b5c8bab5-6ab5-4437-a5fa-e67e9a0169a9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.utils.benchmark.utils.common.Measurement object at 0x7f23d36c92d0>\n",
      "batched_dot_mul_sum(x, x)\n",
      "setup: from __main__ import batched_dot_mul_sum\n",
      "  161.18 us\n",
      "  1 measurement, 100 runs , 1 thread\n",
      "<torch.utils.benchmark.utils.common.Measurement object at 0x7f23d39749d0>\n",
      "batched_dot_bmm(x, x)\n",
      "setup: from __main__ import batched_dot_bmm\n",
      "  540.19 us\n",
      "  1 measurement, 100 runs , 1 thread\n"
     ]
    }
   ],
   "source": [
    "import torch.utils.benchmark as benchmark\n",
    "\n",
    "t0 = benchmark.Timer(\n",
    "    stmt=\"batched_dot_mul_sum(x, x)\",\n",
    "    setup=\"from __main__ import batched_dot_mul_sum\",\n",
    "    globals={\"x\": x},\n",
    ")\n",
    "\n",
    "t1 = benchmark.Timer(\n",
    "    stmt=\"batched_dot_bmm(x, x)\",\n",
    "    setup=\"from __main__ import batched_dot_bmm\",\n",
    "    globals={\"x\": x},\n",
    ")\n",
    "\n",
    "print(t0.timeit(100))\n",
    "print(t1.timeit(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jHfn-xHLLTxm",
    "outputId": "d86285dd-eb94-4f4a-c43d-05842ae2015d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking on 24 threads\n",
      "<torch.utils.benchmark.utils.common.Measurement object at 0x7f23d339f310>\n",
      "Multithreaded batch dot: Implemented using mul and sum\n",
      "setup: from __main__ import batched_dot_mul_sum\n",
      "  2.94 ms\n",
      "  1 measurement, 100 runs , 24 threads\n",
      "<torch.utils.benchmark.utils.common.Measurement object at 0x7f23d339cd10>\n",
      "Multithreaded batch dot: Implemented using bmm\n",
      "setup: from __main__ import batched_dot_bmm\n",
      "  66.05 us\n",
      "  1 measurement, 100 runs , 24 threads\n"
     ]
    }
   ],
   "source": [
    "# in addition, we can set the number of threads for CPU computations\n",
    "num_threads = torch.get_num_threads()\n",
    "print(f\"Benchmarking on {num_threads} threads\")\n",
    "\n",
    "t0 = benchmark.Timer(\n",
    "    stmt=\"batched_dot_mul_sum(x, x)\",\n",
    "    setup=\"from __main__ import batched_dot_mul_sum\",\n",
    "    globals={\"x\": x},\n",
    "    num_threads=num_threads,\n",
    "    label=\"Multithreaded batch dot\",\n",
    "    sub_label=\"Implemented using mul and sum\",\n",
    ")\n",
    "\n",
    "t1 = benchmark.Timer(\n",
    "    stmt=\"batched_dot_bmm(x, x)\",\n",
    "    setup=\"from __main__ import batched_dot_bmm\",\n",
    "    globals={\"x\": x},\n",
    "    num_threads=num_threads,\n",
    "    label=\"Multithreaded batch dot\",\n",
    "    sub_label=\"Implemented using bmm\",\n",
    ")\n",
    "\n",
    "print(t0.timeit(100))\n",
    "print(t1.timeit(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xuHOmnUplY5T",
    "outputId": "1bbe05f9-8ccd-4e0e-ea07-de713ec38050"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking on 2 threads\n",
      "<torch.utils.benchmark.utils.common.Measurement object at 0x7f2416e2ea90>\n",
      "Multithreaded batch dot: Implemented using mul and sum\n",
      "setup: from __main__ import batched_dot_mul_sum\n",
      "  115.11 us\n",
      "  1 measurement, 100 runs , 2 threads\n",
      "<torch.utils.benchmark.utils.common.Measurement object at 0x7f2416e2e490>\n",
      "Multithreaded batch dot: Implemented using bmm\n",
      "setup: from __main__ import batched_dot_bmm\n",
      "  346.68 us\n",
      "  1 measurement, 100 runs , 2 threads\n"
     ]
    }
   ],
   "source": [
    "# we can change it globally for PyTorch and measure the impact\n",
    "prev_num_threads = num_threads\n",
    "torch.set_num_threads(2)\n",
    "\n",
    "num_threads = torch.get_num_threads()\n",
    "print(f\"Benchmarking on {num_threads} threads\")\n",
    "\n",
    "t0 = benchmark.Timer(\n",
    "    stmt=\"batched_dot_mul_sum(x, x)\",\n",
    "    setup=\"from __main__ import batched_dot_mul_sum\",\n",
    "    globals={\"x\": x},\n",
    "    num_threads=num_threads,\n",
    "    label=\"Multithreaded batch dot\",\n",
    "    sub_label=\"Implemented using mul and sum\",\n",
    ")\n",
    "\n",
    "t1 = benchmark.Timer(\n",
    "    stmt=\"batched_dot_bmm(x, x)\",\n",
    "    setup=\"from __main__ import batched_dot_bmm\",\n",
    "    globals={\"x\": x},\n",
    "    num_threads=num_threads,\n",
    "    label=\"Multithreaded batch dot\",\n",
    "    sub_label=\"Implemented using bmm\",\n",
    ")\n",
    "\n",
    "print(t0.timeit(100))\n",
    "print(t1.timeit(100))\n",
    "# in this case, we don't get any speedup, likely due to the overhead\n",
    "\n",
    "torch.set_num_threads(prev_num_threads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0sbnCapQNirw",
    "outputId": "d6066fbd-e2ef-440c-8c5a-2262a70d0cca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Architecture:            x86_64\n",
      "  CPU op-mode(s):        32-bit, 64-bit\n",
      "  Address sizes:         43 bits physical, 48 bits virtual\n",
      "  Byte Order:            Little Endian\n",
      "CPU(s):                  48\n",
      "  On-line CPU(s) list:   0-47\n",
      "Vendor ID:               AuthenticAMD\n",
      "  Model name:            AMD EPYC 7352 24-Core Processor\n",
      "    CPU family:          23\n",
      "    Model:               49\n",
      "    Thread(s) per core:  2\n",
      "    Core(s) per socket:  24\n",
      "    Socket(s):           1\n",
      "    Stepping:            0\n",
      "    Frequency boost:     enabled\n",
      "    CPU max MHz:         2300.0000\n",
      "    CPU min MHz:         1500.0000\n",
      "    BogoMIPS:            4599.57\n",
      "    Flags:               fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mc\n",
      "                         a cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall n\n",
      "                         x mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_go\n",
      "                         od nopl nonstop_tsc cpuid extd_apicid aperfmperf rapl p\n",
      "                         ni pclmulqdq monitor ssse3 fma cx16 sse4_1 sse4_2 movbe\n",
      "                          popcnt aes xsave avx f16c rdrand lahf_lm cmp_legacy sv\n",
      "                         m extapic cr8_legacy abm sse4a misalignsse 3dnowprefetc\n",
      "                         h osvw ibs skinit wdt tce topoext perfctr_core perfctr_\n",
      "                         nb bpext perfctr_llc mwaitx cpb cat_l3 cdp_l3 hw_pstate\n",
      "                          ssbd mba ibrs ibpb stibp vmmcall fsgsbase bmi1 avx2 sm\n",
      "                         ep bmi2 cqm rdt_a rdseed adx smap clflushopt clwb sha_n\n",
      "                         i xsaveopt xsavec xgetbv1 cqm_llc cqm_occup_llc cqm_mbm\n",
      "                         _total cqm_mbm_local clzero irperf xsaveerptr rdpru wbn\n",
      "                         oinvd amd_ppin arat npt lbrv svm_lock nrip_save tsc_sca\n",
      "                         le vmcb_clean flushbyasid decodeassists pausefilter pft\n",
      "                         hreshold avic v_vmsave_vmload vgif v_spec_ctrl umip rdp\n",
      "                         id overflow_recov succor smca sme sev sev_es\n",
      "Virtualization features: \n",
      "  Virtualization:        AMD-V\n",
      "Caches (sum of all):     \n",
      "  L1d:                   768 KiB (24 instances)\n",
      "  L1i:                   768 KiB (24 instances)\n",
      "  L2:                    12 MiB (24 instances)\n",
      "  L3:                    128 MiB (8 instances)\n",
      "NUMA:                    \n",
      "  NUMA node(s):          1\n",
      "  NUMA node0 CPU(s):     0-47\n",
      "Vulnerabilities:         \n",
      "  Gather data sampling:  Not affected\n",
      "  Itlb multihit:         Not affected\n",
      "  L1tf:                  Not affected\n",
      "  Mds:                   Not affected\n",
      "  Meltdown:              Not affected\n",
      "  Mmio stale data:       Not affected\n",
      "  Retbleed:              Mitigation; untrained return thunk; SMT enabled with ST\n",
      "                         IBP protection\n",
      "  Spec rstack overflow:  Mitigation; safe RET\n",
      "  Spec store bypass:     Mitigation; Speculative Store Bypass disabled via prctl\n",
      "                          and seccomp\n",
      "  Spectre v1:            Mitigation; usercopy/swapgs barriers and __user pointer\n",
      "                          sanitization\n",
      "  Spectre v2:            Mitigation; Retpolines; IBPB conditional; STIBP always-\n",
      "                         on; RSB filling; PBRSB-eIBRS Not affected; BHI Not affe\n",
      "                         cted\n",
      "  Srbds:                 Not affected\n",
      "  Tsx async abort:       Not affected\n"
     ]
    }
   ],
   "source": [
    "# by the way, what CPU do we have?\n",
    "!lscpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KHJDeL81zJt4"
   },
   "source": [
    "## Benchmarking GPU code\n",
    "As we discussed in the lecture, CUDA kernel execution and PyTorch GPU operations are asynchronous.\n",
    "This means that it is possible to launch several kernels in a row before receiving results from the first one. As a consequence, naive benchmarking without synchronization is likely to give you unrealistic results.\n",
    "\n",
    "Let's the same example, but with slightly larger matrices on the GPU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UWPVFvQxLWk9",
    "outputId": "516bd654-15ae-413a-db06-81f4223291bb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mul_sum(x, x):  192.3 us\n",
      "mul_sum(x, x):   22.7 us\n",
      "bmm(x, x):      648.5 us\n",
      "bmm(x, x):       23.9 us\n"
     ]
    }
   ],
   "source": [
    "import timeit\n",
    "\n",
    "x = torch.randn(10000, 1024, device=\"cuda\")\n",
    "\n",
    "t0 = timeit.Timer(\n",
    "    stmt=\"batched_dot_mul_sum(x, x)\",\n",
    "    setup=\"from __main__ import batched_dot_mul_sum\",\n",
    "    globals={\"x\": x},\n",
    ")\n",
    "\n",
    "t1 = timeit.Timer(\n",
    "    stmt=\"batched_dot_bmm(x, x)\",\n",
    "    setup=\"from __main__ import batched_dot_bmm\",\n",
    "    globals={\"x\": x},\n",
    ")\n",
    "\n",
    "# Ran each twice to show difference before/after warmup\n",
    "print(f\"mul_sum(x, x):  {t0.timeit(100) / 100 * 1e6:>5.1f} us\")\n",
    "print(f\"mul_sum(x, x):  {t0.timeit(100) / 100 * 1e6:>5.1f} us\")\n",
    "print(f\"bmm(x, x):      {t1.timeit(100) / 100 * 1e6:>5.1f} us\")\n",
    "print(f\"bmm(x, x):      {t1.timeit(100) / 100 * 1e6:>5.1f} us\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xt5nFXZJzn-q"
   },
   "source": [
    "First, we see that the difference between the first and the second runs of timeit is quite noticeable, which happens because of initializing the CUDA context and loading [cuBLAS](https://docs.nvidia.com/cuda/cublas/index.html) — a CUDA library for accelerated linear algebra.\n",
    "\n",
    "Second, the runtimes of two methods seem too small.\n",
    "\n",
    "Let's run the same test with `torch.utils.benchmark` to see a different set of results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "G2S71HcljicI",
    "outputId": "7307b6a4-b6cb-4f9e-c729-a8da3e28e436"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.utils.benchmark.utils.common.Measurement object at 0x7f23d3398290>\n",
      "batched_dot_mul_sum(x, x)\n",
      "setup: from __main__ import batched_dot_mul_sum\n",
      "  322.92 us\n",
      "  1 measurement, 100 runs , 1 thread\n",
      "<torch.utils.benchmark.utils.common.Measurement object at 0x7f23d3391f50>\n",
      "batched_dot_bmm(x, x)\n",
      "setup: from __main__ import batched_dot_bmm\n",
      "  50.70 us\n",
      "  1 measurement, 100 runs , 1 thread\n"
     ]
    }
   ],
   "source": [
    "t0 = benchmark.Timer(\n",
    "    stmt=\"batched_dot_mul_sum(x, x)\",\n",
    "    setup=\"from __main__ import batched_dot_mul_sum\",\n",
    "    globals={\"x\": x},\n",
    ")\n",
    "\n",
    "t1 = benchmark.Timer(\n",
    "    stmt=\"batched_dot_bmm(x, x)\",\n",
    "    setup=\"from __main__ import batched_dot_bmm\",\n",
    "    globals={\"x\": x},\n",
    ")\n",
    "\n",
    "# Run only once since benchmark module does warmup for us\n",
    "print(t0.timeit(100))\n",
    "print(t1.timeit(100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k0OG91pQztfY"
   },
   "source": [
    "Now we have a more realistic set of measurements, which is caused by explicitly triggering the CPU-GPU synchronization and awaiting the results after launching the multiplication. The runtime is comparable to what we had in a previous set of benchmarks, but pay attention that now the matrices being multiplied are 16 times bigger.\n",
    "\n",
    "Let's implement microbenchmarking by ourselves using nothing but [`time.perf_counter()`](https://docs.python.org/3/library/time.html#time.perf_counter) and CUDA methods given by PyTorch. First, we'll run `batched_dot_mul_sum` on the GPU with and without synchronization and see how this affects the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gFyWDoTnrkdw",
    "outputId": "e9948dcd-0949-4058-ea2f-8f416f5c99d8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(9.413926862180233e-05)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from time import perf_counter\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "execution_times = []\n",
    "\n",
    "for _ in range(100):\n",
    "    start_time = perf_counter()\n",
    "    batched_dot_mul_sum(x, x)\n",
    "    execution_times.append(perf_counter() - start_time)\n",
    "\n",
    "np.mean(execution_times)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "daRiA7czQSF1"
   },
   "source": [
    "Compare this with the result that does not compute anything:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bhyGF5-Ftta4",
    "outputId": "19f02958-1c09-4681-d8bb-b72c957d7b85"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(3.030803054571152e-07)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "execution_times = []\n",
    "\n",
    "for _ in range(100):\n",
    "    start = perf_counter()\n",
    "    execution_times.append(perf_counter() - start)\n",
    "\n",
    "np.mean(execution_times)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KJ8KEHVKQ1Y_"
   },
   "source": [
    "Finally, let's explicitly call `torch.cuda.synchronize` at the end of each iteration to see the difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kjmFVZdPQjrj",
    "outputId": "bde77f5c-de52-4ec8-aa28-01a3e4e82f82"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.000405278243124485)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "execution_times = []\n",
    "\n",
    "for _ in range(100):\n",
    "    start_time = perf_counter()\n",
    "    batched_dot_mul_sum(x, x)\n",
    "    torch.cuda.synchronize()\n",
    "    execution_times.append(perf_counter() - start_time)\n",
    "\n",
    "np.mean(execution_times)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r4AKp3IrW_ei"
   },
   "source": [
    "The same thing applies to the actual models. Let's take `Linear` as the simplest example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "E21i4UMYujuQ",
    "outputId": "0c40b288-83ce-427d-9bda-64c0bd24c2b6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(5.590742453932762e-05)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(10000, 512, device=\"cuda\")\n",
    "linear = torch.nn.Linear(512, 1024, bias=False, device=\"cuda\")\n",
    "N_ITERS = 200\n",
    "\n",
    "execution_times = []\n",
    "\n",
    "for _ in range(N_ITERS):\n",
    "    start = perf_counter()\n",
    "    result = linear(x)\n",
    "    execution_times.append(perf_counter() - start)\n",
    "\n",
    "np.mean(execution_times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "T6oR06jMvXA3",
    "outputId": "024a1da0-42fe-479a-d030-c9ee10554bb3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.0009008157346397639)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "execution_times = []\n",
    "\n",
    "for _ in range(N_ITERS):\n",
    "    start = perf_counter()\n",
    "    result = linear(x)\n",
    "    torch.cuda.synchronize()\n",
    "    execution_times.append(perf_counter() - start)\n",
    "\n",
    "np.mean(execution_times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bNJ0S1ykwPtZ",
    "outputId": "4c3f9d18-6520-432a-8c6d-4da24d07e94c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0008642355445772409"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "execution_times = []\n",
    "\n",
    "start = perf_counter()\n",
    "for _ in range(N_ITERS):\n",
    "    result = linear(x)\n",
    "torch.cuda.synchronize()\n",
    "\n",
    "(perf_counter() - start) / N_ITERS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7C3AJER9XU0t"
   },
   "source": [
    "Of course, `torch.utils.benchmark` also works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ISGCgjslvqXz",
    "outputId": "8cdc2692-e90f-49f1-ed22-aa06c2f524bc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.utils.benchmark.utils.common.Measurement object at 0x7f2515d39650>\n",
      "linear(x)\n",
      "  859.90 us\n",
      "  1 measurement, 100 runs , 1 thread\n"
     ]
    }
   ],
   "source": [
    "t0 = benchmark.Timer(stmt=\"linear(x)\", globals={\"x\": x, \"linear\": linear})\n",
    "print(t0.timeit(100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j8HSUDAISim3"
   },
   "source": [
    "## CUDA Streams\n",
    "To execute several operations concurrently, you may use CUDA streams in PyTorch. Below, you can see an example of their usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "VAwCkCrgmwLw"
   },
   "outputs": [],
   "source": [
    "cuda = torch.device(\"cuda\")\n",
    "s = torch.cuda.Stream()  # Create a new stream.\n",
    "A = torch.empty((100, 100), device=cuda).normal_(0.0, 1.0)\n",
    "with torch.cuda.stream(s):\n",
    "    # sum() may start execution before normal_() finishes!\n",
    "    B = torch.sum(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fJOJ1bi4nImI",
    "outputId": "be1dd79d-fcc2-4df0-8549-cec3e28bc2d6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.00039998057298362255)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s1 = torch.cuda.Stream()\n",
    "s2 = torch.cuda.Stream()\n",
    "# Initialize cuda tensors here. E.g.:\n",
    "A = torch.rand(1000, 1000, device=\"cuda\")\n",
    "B = torch.rand(1000, 1000, device=\"cuda\")\n",
    "# Wait for the above tensors to initialize.\n",
    "torch.cuda.synchronize()\n",
    "\n",
    "execution_times = []\n",
    "\n",
    "for _ in range(100):\n",
    "    start = perf_counter()\n",
    "    with torch.cuda.stream(s1):\n",
    "        C = torch.mm(A, A)\n",
    "    with torch.cuda.stream(s2):\n",
    "        D = torch.mm(B, B)\n",
    "    # Wait for C and D to be computed.\n",
    "    torch.cuda.synchronize()\n",
    "    execution_times.append(perf_counter() - start)\n",
    "\n",
    "np.mean(execution_times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RRw1SQ9hmNiW",
    "outputId": "feccda2a-2629-4f3b-8e6b-b47c97a36e1a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.00035375642590224744)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# next, let's compute C and D sequentially\n",
    "\n",
    "execution_times = []\n",
    "\n",
    "for _ in range(100):\n",
    "    start = perf_counter()\n",
    "    C = torch.mm(A, A)\n",
    "    D = torch.mm(B, B)\n",
    "    # Wait for C and D to be computed.\n",
    "    torch.cuda.synchronize()\n",
    "    execution_times.append(perf_counter() - start)\n",
    "\n",
    "np.mean(execution_times)\n",
    "# the speed is even higher in this case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iog1q8KTuYtx"
   },
   "source": [
    "As you can see in this example, the usefulness of streams can be limited: concurrent kernels need to underutilize the GPU and yet take long enough to compute. It might still be a good idea if you are trying to compute some expression and fetch data for the next computation simultaneously (for example, when training examples are large or in case of offloading).\n",
    "The example below demonstrates how to use streams for that purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "3dR-9sTJjbsP"
   },
   "outputs": [],
   "source": [
    "compute_stream = torch.cuda.Stream()\n",
    "h2d_stream = torch.cuda.Stream()\n",
    "\n",
    "A = torch.rand(10240, 10240, device=\"cuda\")\n",
    "B = [torch.rand(4096, 4096, device=\"cpu\").pin_memory() for _ in range(100)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "5fGIhMiNr5zF"
   },
   "outputs": [],
   "source": [
    "def sequential_execution():\n",
    "  torch.matmul(A, A)\n",
    "  for matrix in B:\n",
    "    matrix.to(\"cuda\", non_blocking=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nW1luh_cj3QZ",
    "outputId": "8890948c-4ebe-407b-acd8-77c38c103eb2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "385 ms ± 810 μs per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "sequential_execution()\n",
    "torch.cuda.synchronize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "2HpXlmGtj98h"
   },
   "outputs": [],
   "source": [
    "def stream_execution():\n",
    "  with torch.cuda.stream(compute_stream):\n",
    "    torch.matmul(A, A)\n",
    "  with torch.cuda.stream(h2d_stream):\n",
    "    for matrix in B:\n",
    "      matrix.to(\"cuda\", non_blocking=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "E5S06upZkF93",
    "outputId": "6e5fab5d-8ad0-4a13-b648-a09f0558fc69"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "255 ms ± 15.5 μs per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "stream_execution()\n",
    "torch.cuda.synchronize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TC5UISMiSAfs"
   },
   "source": [
    "## Debugging asynchronous code\n",
    "Finding sources of errors in GPU-reliant code in PyTorch can also be difficult. Let's see this on a standard example of an off-by-one error and incorrect index for the embedding layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "75YMFmOdzhAw",
    "outputId": "488b5403-cd24-4e73-9131-affff705fc81"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing incorrect_index.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile incorrect_index.py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "embedding = nn.Embedding(1024,32).to('cuda')\n",
    "# 1024 > 1023 (largest index in the created embedding layer)\n",
    "input = torch.full((1,1),1024,dtype=torch.long, device='cuda')\n",
    "\n",
    "# out-of-bounds access\n",
    "embedding_for_index = embedding(input)\n",
    "\n",
    "result = torch.sigmoid(embedding_for_index)\n",
    "loss = result.sum()\n",
    "print(loss.item())\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dxMcIkDmYBFn"
   },
   "source": [
    "If we attempt to run the code as is, we'll see an error after the point in which we trigger the CPU-GPU synchronization (`loss.item`). This gives us no clues about what exactly caused an error:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "whEGTI1-VVQ6",
    "outputId": "53896f45-fb12-48b0-8e93-d486a7379b3b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/workspace/efficient-dl-systems/week01_intro/incorrect_index.py\", line 11, in <module>\n",
      "    result = torch.sigmoid(embedding_for_index)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "RuntimeError: CUDA error: device-side assert triggered\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1231: indexSelectSmallIndex: block: [0,0,0], thread: [0,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1231: indexSelectSmallIndex: block: [0,0,0], thread: [1,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1231: indexSelectSmallIndex: block: [0,0,0], thread: [2,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1231: indexSelectSmallIndex: block: [0,0,0], thread: [3,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1231: indexSelectSmallIndex: block: [0,0,0], thread: [4,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1231: indexSelectSmallIndex: block: [0,0,0], thread: [5,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1231: indexSelectSmallIndex: block: [0,0,0], thread: [6,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1231: indexSelectSmallIndex: block: [0,0,0], thread: [7,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1231: indexSelectSmallIndex: block: [0,0,0], thread: [8,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1231: indexSelectSmallIndex: block: [0,0,0], thread: [9,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1231: indexSelectSmallIndex: block: [0,0,0], thread: [10,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1231: indexSelectSmallIndex: block: [0,0,0], thread: [11,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1231: indexSelectSmallIndex: block: [0,0,0], thread: [12,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1231: indexSelectSmallIndex: block: [0,0,0], thread: [13,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1231: indexSelectSmallIndex: block: [0,0,0], thread: [14,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1231: indexSelectSmallIndex: block: [0,0,0], thread: [15,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1231: indexSelectSmallIndex: block: [0,0,0], thread: [16,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1231: indexSelectSmallIndex: block: [0,0,0], thread: [17,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1231: indexSelectSmallIndex: block: [0,0,0], thread: [18,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1231: indexSelectSmallIndex: block: [0,0,0], thread: [19,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1231: indexSelectSmallIndex: block: [0,0,0], thread: [20,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1231: indexSelectSmallIndex: block: [0,0,0], thread: [21,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1231: indexSelectSmallIndex: block: [0,0,0], thread: [22,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1231: indexSelectSmallIndex: block: [0,0,0], thread: [23,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1231: indexSelectSmallIndex: block: [0,0,0], thread: [24,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1231: indexSelectSmallIndex: block: [0,0,0], thread: [25,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1231: indexSelectSmallIndex: block: [0,0,0], thread: [26,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1231: indexSelectSmallIndex: block: [0,0,0], thread: [27,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1231: indexSelectSmallIndex: block: [0,0,0], thread: [28,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1231: indexSelectSmallIndex: block: [0,0,0], thread: [29,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1231: indexSelectSmallIndex: block: [0,0,0], thread: [30,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1231: indexSelectSmallIndex: block: [0,0,0], thread: [31,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n"
     ]
    }
   ],
   "source": [
    "!python incorrect_index.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SK-kBXycW0c-"
   },
   "source": [
    "(though an experienced person might get a hint from the C++ failed assertions printed before the error)\n",
    "\n",
    "There are two ways to find the actual line that caused an exception:\n",
    "\n",
    "* First, you can just move everything to CPU. This is a valid approach, but it involves making changes to your code or input arguments and sometimes can be difficult (for example, if the error occurs after a long chain of operations).\n",
    "\n",
    "* Second, you may use the `CUDA_LAUNCH_BLOCKING` environment variable when starting your code. This will force synchronization for all GPU-related operations, making your code slower but allowing to see the exact source of the error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HF8lRP3_0XY0",
    "outputId": "eed4119a-4a1e-44d2-bf8c-c34103dc25a2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../aten/src/ATen/native/cuda/Indexing.cu:1231: indexSelectSmallIndex: block: [0,0,0], thread: [0,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1231: indexSelectSmallIndex: block: [0,0,0], thread: [1,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1231: indexSelectSmallIndex: block: [0,0,0], thread: [2,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1231: indexSelectSmallIndex: block: [0,0,0], thread: [3,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1231: indexSelectSmallIndex: block: [0,0,0], thread: [4,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1231: indexSelectSmallIndex: block: [0,0,0], thread: [5,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1231: indexSelectSmallIndex: block: [0,0,0], thread: [6,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1231: indexSelectSmallIndex: block: [0,0,0], thread: [7,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1231: indexSelectSmallIndex: block: [0,0,0], thread: [8,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1231: indexSelectSmallIndex: block: [0,0,0], thread: [9,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1231: indexSelectSmallIndex: block: [0,0,0], thread: [10,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1231: indexSelectSmallIndex: block: [0,0,0], thread: [11,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1231: indexSelectSmallIndex: block: [0,0,0], thread: [12,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1231: indexSelectSmallIndex: block: [0,0,0], thread: [13,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1231: indexSelectSmallIndex: block: [0,0,0], thread: [14,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1231: indexSelectSmallIndex: block: [0,0,0], thread: [15,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1231: indexSelectSmallIndex: block: [0,0,0], thread: [16,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1231: indexSelectSmallIndex: block: [0,0,0], thread: [17,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1231: indexSelectSmallIndex: block: [0,0,0], thread: [18,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1231: indexSelectSmallIndex: block: [0,0,0], thread: [19,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1231: indexSelectSmallIndex: block: [0,0,0], thread: [20,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1231: indexSelectSmallIndex: block: [0,0,0], thread: [21,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1231: indexSelectSmallIndex: block: [0,0,0], thread: [22,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1231: indexSelectSmallIndex: block: [0,0,0], thread: [23,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1231: indexSelectSmallIndex: block: [0,0,0], thread: [24,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1231: indexSelectSmallIndex: block: [0,0,0], thread: [25,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1231: indexSelectSmallIndex: block: [0,0,0], thread: [26,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1231: indexSelectSmallIndex: block: [0,0,0], thread: [27,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1231: indexSelectSmallIndex: block: [0,0,0], thread: [28,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1231: indexSelectSmallIndex: block: [0,0,0], thread: [29,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1231: indexSelectSmallIndex: block: [0,0,0], thread: [30,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1231: indexSelectSmallIndex: block: [0,0,0], thread: [31,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "Traceback (most recent call last):\n",
      "  File \"/workspace/efficient-dl-systems/week01_intro/incorrect_index.py\", line 9, in <module>\n",
      "    embedding_for_index = embedding(input)\n",
      "                          ^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/sparse.py\", line 164, in forward\n",
      "    return F.embedding(\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/functional.py\", line 2267, in embedding\n",
      "    return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "RuntimeError: CUDA error: device-side assert triggered\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!CUDA_LAUNCH_BLOCKING=1 python incorrect_index.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zFDw7mEe0xe1"
   },
   "source": [
    "## Precision of floating point operations\n",
    "Let's have a look at this code snippet and its results. In essence, it computes the sixth power of a random matrix (with a fixed seed) on two different devices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rNzil3C50_yX",
    "outputId": "8da9780a-3212-4918-b453-86b4d70873cd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27654759645184.0\n",
      "27654807879680.0\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "x = torch.randn(5000, 5000)\n",
    "torch.use_deterministic_algorithms(False)\n",
    "\n",
    "\n",
    "def matrix_power(x):\n",
    "    y = x @ x @ x @ x @ x @ x\n",
    "    return (y).sum().item()\n",
    "\n",
    "\n",
    "print(matrix_power(x))\n",
    "print(matrix_power(x.cuda()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sCl7Jn5Q6gm_"
   },
   "source": [
    "(in case you are wondering, `torch.use_deterministic_algorithms(True)` won't help)\n",
    "\n",
    "If we do the same using numpy (in two ways), we also get a different result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DDNNwEX72_Bp",
    "outputId": "b1e496fc-0724-4fdf-9a3c-e18befaa1ba7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27654747062272.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "np.float32(27654768000000.0)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(matrix_power(x.numpy()))\n",
    "np.linalg.matrix_power(x.numpy(), 6).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mgGAaBl16uka"
   },
   "source": [
    "Takeaway: numerical precision of floating point computations can vary between libraries, environments and devices, and from the user side, it is often hard to resolve this issue altogether. Usually, this happens due to a different summation order in code or due to inherent nondeterminism of hardware.\n",
    "\n",
    "However, note that the relative error is small enough, which makes such blatant discrepancies less of a problem in regular deep learning code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KE29v0NDvTup"
   },
   "source": [
    "## CUDA Graphs\n",
    "\n",
    "In some situations, the bottleneck of your code might be not the GPU compute performance and not even the memory bandwidth, but the latency of kernel execution. Launching each kernel takes CPU time, and if the kernels themselves run very fast, these launches can become a problem.\n",
    "\n",
    "Fortunately, [CUDA graphs](https://github.com/pytorch.org/blog/accelerating-pytorch-with-cuda-graphs/) can address this problem. CUDA graphs capture a sequence of specific operations (i.e., kernel launches) that can later be replayed on new inputs with essentially a single kernel launch. Below, you can see an example of using graphs with a function that consists of many fast-running operations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LEq0c-w0n4Ze",
    "outputId": "ad12ae75-705f-420d-9fb1-42252f53a771"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([6.0000, 6.0000, 6.0000, 6.0000, 6.0000], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "def slow_function(x):\n",
    "  for _ in range(500):\n",
    "    y = x*2\n",
    "    z = torch.sigmoid(y)\n",
    "    a = z + 5.0\n",
    "    b = torch.nn.functional.relu(a)\n",
    "    x = b\n",
    "  return b\n",
    "\n",
    "# the code below is a modified version of https://pytorch.org/docs/master/notes/cuda.html#cuda-graph-semantics\n",
    "g = torch.cuda.CUDAGraph()\n",
    "\n",
    "# Placeholder input used for capture\n",
    "static_input = torch.ones((5,), device=\"cuda\")\n",
    "print(slow_function(static_input))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1dDnX3EKPiGb"
   },
   "source": [
    "Below, you can see two examples of running complex_function: directly (the standard way) or by capturing `func_graph`, copying new inputs into `static_input` and `replay()`ing it. As you can see, the second version can be alsmost 10x faster compared to default eager execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "k1HOYoQQoULd",
    "outputId": "9bb0ee1d-9ee1-4281-cfbc-d241d63dc8cf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18.3 ms ± 11.5 μs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "slow_function(static_input)\n",
    "torch.cuda.synchronize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GGIpgzjgoma-",
    "outputId": "dbab4691-78ac-42b9-f1ce-8b2441cf3529"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([6.0000, 6.0000, 6.0000, 6.0000, 6.0000], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "func_graph = torch.cuda.CUDAGraph()\n",
    "with torch.cuda.graph(func_graph):\n",
    "  static_output = slow_function(static_input)\n",
    "static_output.zero_()\n",
    "\n",
    "# Fills the graph's input memory with new data to compute on\n",
    "static_input.copy_(torch.full((5,), 3, device=\"cuda\"))\n",
    "func_graph.replay()\n",
    "print(static_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IBD4zgNRo1Qg",
    "outputId": "e3754614-17d9-40fa-de0b-a44d74747e4d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.66 ms ± 529 ns per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "func_graph.replay()\n",
    "torch.cuda.synchronize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oD3lU1o6ZeUR"
   },
   "source": [
    "# Bonus (2/10 points for the first home assignment block)\n",
    "Solve exercises in the notebook from https://github.com/srush/GPU-Puzzles. You will earn the number of points corresponding to the fraction of completed exercises."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "9UtSWLdqVeoM"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
